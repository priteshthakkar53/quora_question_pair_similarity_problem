{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import distance\n",
    "from fuzzywuzzy import fuzz\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from tqdm.notebook import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading file\n",
    "df = pd.read_csv('train.csv')\n",
    "print('Number of data points: {}'.format(df.shape[0]))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting data points in each class\n",
    "df.groupby('is_duplicate')['id'].count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of question pairs: {}'.format(df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Similar questions (is_duplicate=1): {}%'.format(round(((df.groupby('is_duplicate')['id'].count()[1]) / (df.shape[0]))*100, 2)))\n",
    "print('Dissimilar questions (is_duplicate=0): {}%'.format(100-round(((df.groupby('is_duplicate')['id'].count()[1]) / (df.shape[0]))*100, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = pd.Series(df['qid1'].to_list() + df['qid2'].to_list())\n",
    "\n",
    "unique_qs = len(np.unique(qids))\n",
    "print('Total number of unique questions: {}'.format(unique_qs))\n",
    "\n",
    "qs_morethan_onetime = np.sum(qids.value_counts()>1)\n",
    "print('Questions that appear more than one time: {}'.format(qs_morethan_onetime))\n",
    "\n",
    "print('Max number of time a single question is repeated: {}'.format(np.max(qids.value_counts())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ['Unique Questions', 'Repeated Questions']\n",
    "y = [unique_qs, qs_morethan_onetime]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.title('Plot representing unique and repeated questions')\n",
    "sns.barplot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if we have duplicate data point\n",
    "pair_duplicates = df[['qid1','qid2','is_duplicate']].groupby(['qid1','qid2']).count().reset_index()\n",
    "print('Duplicate data points: {}'.format(df.shape[0]-pair_duplicates.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.hist(qids.value_counts(), bins=160)\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.title('Log-Histogram of question appearance count')\n",
    "plt.xlabel('Number of occurance')\n",
    "plt.ylabel('Number of questions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking nan rows\n",
    "nan_rows = df[df.isnull().values]\n",
    "print(nan_rows)\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating target variable\n",
    "X = df.iloc[:,:5]\n",
    "y = df.iloc[:,-1]\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating train and test datasets\n",
    "if os.path.isfile('quora_train.csv'):\n",
    "    df = pd.read_csv('quora_train.csv', encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)\n",
    "    train = [X_train, y_train]\n",
    "    test = [X_test, y_test]\n",
    "    df_train = pd.concat(train, axis=1)\n",
    "    df_test = pd.concat(test, axis=1)\n",
    "    df_train.to_csv('quora_train.csv', index=False)\n",
    "    df_test.to_csv('quora_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('quora_fe_without_preprocessing.csv'):\n",
    "    df = pd.read_csv('quora_fe_without_preprocessing.csv', encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "else:\n",
    "    df = pd.read_csv('quora_train.csv', encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "    \n",
    "    #qid1_freq: frequency of qid1\n",
    "    df['qid1_freq'] = df.groupby('qid1')['qid1'].transform('count')\n",
    "    \n",
    "    #qid2_freq: frequency of qid2\n",
    "    df['qid2_freq'] = df.groupby('qid2')['qid2'].transform('count')\n",
    "    \n",
    "    #q1_len: length of question1\n",
    "    df['q1_len'] = df['question1'].str.len()\n",
    "    \n",
    "    #q2_len: length of question2\n",
    "    df['q2_len'] = df['question2'].str.len()\n",
    "    \n",
    "    #q1_n_words: number of words in question1\n",
    "    df['q1_n_words'] = df['question1'].apply(lambda row: len(row.split(' ')))\n",
    "    \n",
    "    #q2_n_words: number of words in question2\n",
    "    df['q2_n_words'] = df['question2'].apply(lambda row: len(row.split(' ')))\n",
    "    \n",
    "    #common_words: number of common unique words in question1 and question2\n",
    "    def normalized_common_words(row):\n",
    "        w1 = set(map(lambda row: row.lower().strip(), row['question1'].split(' ')))\n",
    "        w2 = set(map(lambda row: row.lower().strip(), row['question2'].split(' ')))\n",
    "        return 1.0 * len(w1 & w2)\n",
    "    df['common_words'] = df.apply(normalized_common_words, axis=1)\n",
    "    \n",
    "    #total_words: total number of words in question1 and question2\n",
    "    def normalized_word_total(row):\n",
    "        w1 = set(map(lambda row: row.lower().strip(), row['question1'].split(' ')))\n",
    "        w2 = set(map(lambda row: row.lower().strip(), row['question2'].split(' ')))\n",
    "        return 1.0 * (len(w1) + len(w2))\n",
    "    df['total_words'] = df.apply(normalized_word_total, axis=1)\n",
    "    \n",
    "    #shared_words: common_words/total_words\n",
    "    df['shared_words'] = df['common_words']/df['total_words']\n",
    "    \n",
    "    #qid1+qid2_freq: sum of frequencies of qid1 and qid2\n",
    "    df['qid1+qid2_freq'] = df['qid1_freq']+df['qid2_freq']\n",
    "    \n",
    "    #qid1-qid2_freq: absolute difference of frequencies of qid1 and qid2\n",
    "    df['qid1-qid2_freq'] = abs(df['qid1_freq']-df['qid2_freq'])\n",
    "    \n",
    "    df.to_csv('quora_fe_without_preprocessing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Minumum length of questions in question1: {}'.format(min(df['q1_n_words'])))\n",
    "print('Minimum length of questions in question2: {}'.format(min(df['q2_n_words'])))\n",
    "\n",
    "print('Number of questions with minimum length in question1: {}'.format(df[df['q1_n_words']==1].shape[0]))\n",
    "print('Number of questions with minimum length in question2: {}'.format(df[df['q2_n_words']==1].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x='is_duplicate', y='shared_words', data=df)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate']==1]['shared_words'], color='red')\n",
    "sns.distplot(df[df['is_duplicate']==0]['shared_words'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x='is_duplicate', y='common_words', data=df)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate']==1]['common_words'], color='red')\n",
    "sns.distplot(df[df['is_duplicate']==0]['common_words'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAFE_DIV = 0.0001\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "def pre_process(col):\n",
    "    col = str(col).lower()\n",
    "    col = col.replace(\",000,000\", \"m\")\\\n",
    "        .replace(\",000\", \"k\")\\\n",
    "        .replace(\"′\", \"'\")\\\n",
    "        .replace(\"’\", \"'\")\\\n",
    "        .replace(\"won't\", \"will not\")\\\n",
    "        .replace(\"cannot\", \"can not\")\\\n",
    "        .replace(\"can't\", \"can not\")\\\n",
    "        .replace(\"n't\", \" not\")\\\n",
    "        .replace(\"what's\", \"what is\")\\\n",
    "        .replace(\"it's\", \"it is\")\\\n",
    "        .replace(\"'ve\", \" have\")\\\n",
    "        .replace(\"i'm\", \"i am\")\\\n",
    "        .replace(\"'re\", \" are\")\\\n",
    "        .replace(\"he's\", \"he is\")\\\n",
    "        .replace(\"she's\", \"she is\")\\\n",
    "        .replace(\"'s\", \" own\")\\\n",
    "        .replace(\"%\", \" percent \")\\\n",
    "        .replace(\"₹\", \" rupee \")\\\n",
    "        .replace(\"$\", \" dollar \")\\\n",
    "        .replace(\"€\", \" euro \")\\\n",
    "        .replace(\"'ll\", \" will\")\n",
    "    \n",
    "    col = re.sub(r\"([0-9]+)000000\", r\"\\1m\", col)\n",
    "    col = re.sub(r\"([0-9]+)000\", r\"\\1k\", col)\n",
    "    \n",
    "    porter = PorterStemmer()\n",
    "    \n",
    "    # '\\W' matches any non aplhanumeric charecter\n",
    "    pattern = re.compile('\\W')\n",
    "    \n",
    "    if type(col)==type(''):\n",
    "        col = re.sub(pattern, ' ', col)\n",
    "    \n",
    "    if type(col)==type(''):\n",
    "        col=porter.stem(col)\n",
    "        text = BeautifulSoup(col)\n",
    "        col = text.get_text()\n",
    "    \n",
    "    return col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_features(q1, q2):\n",
    "    \n",
    "    # create a list of size 10 and initialize each element with 0\n",
    "    token_features = [0.0]*10\n",
    "    \n",
    "    # convert sentence to tokens\n",
    "    q1_tokens = q1.split(' ')\n",
    "    q2_tokens = q2.split(' ')\n",
    "    \n",
    "    if len(q1_tokens)==0 or len(q2_tokens)==0:\n",
    "        return token_features\n",
    "    \n",
    "    # get words other than stopwords\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    # get stopwords\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    \n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    common_words_count = len(q1_words.intersection(q2_words))\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # cwc_min: ratio of common words to minimum length of word count of question1 and question2\n",
    "    token_features[0] = common_words_count/(min(len(q1_words),len(q2_words))+SAFE_DIV)\n",
    "    # cwc_max: ratio of common words to maximum length of word count of question1 and question2\n",
    "    token_features[1] = common_words_count/(max(len(q1_words),len(q2_words))+SAFE_DIV)\n",
    "    \n",
    "    # csc_min: ratio of common stop words to minimum length of stop count of question1 and question2\n",
    "    token_features[2] = common_stop_count/(min(len(q1_stops),len(q2_stops))+SAFE_DIV)\n",
    "    # csc_max: ratio of common stop words to maximum length of stop count of question1 and question2\n",
    "    token_features[3] = common_stop_count/(max(len(q1_stops),len(q2_stops))+SAFE_DIV)\n",
    "    \n",
    "    # ctc_min: ratio of common token words to minimum length of token count of question1 and question2\n",
    "    token_features[4] = common_token_count/(min(len(q1_tokens),len(q2_tokens))+SAFE_DIV)\n",
    "    # ctc_max: ratio of common token words to maximum length of token count of question1 and question2\n",
    "    token_features[5] = common_token_count/(max(len(q1_tokens),len(q2_tokens))+SAFE_DIV)\n",
    "    \n",
    "    # last_word_eq\n",
    "    token_features[6] = int(q1_tokens[-1]==q2_tokens[-1])\n",
    "    \n",
    "    # first_word_eq\n",
    "    token_features[7] = int(q1_tokens[0]==q2_tokens[0])\n",
    "    \n",
    "    # abs_len_diff\n",
    "    token_features[8] = abs(len(q1_tokens)-len(q2_tokens))\n",
    "    \n",
    "    # mean_length\n",
    "    token_features[9] = (len(q1_tokens)+len(q2_tokens))/2\n",
    "    \n",
    "    return token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# longest_substr_ratio: len(longest common substring) / (min(len(q1_tokens), len(q2_tokens)))\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a,b))\n",
    "    if len(strs)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0])/min(len(a),len(b),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    df['question1'] = df['question1'].apply(pre_process)\n",
    "    df['question2'] = df['question2'].apply(pre_process)\n",
    "    \n",
    "    token_features = df.apply(lambda x: get_token_features(x['question1'],x['question2']), axis=1)\n",
    "    \n",
    "    df['cwc_min']       = list(map(lambda x: x[0], token_features))\n",
    "    df['cwc_max']       = list(map(lambda x: x[1], token_features))\n",
    "    df['csc_min']       = list(map(lambda x: x[2], token_features))\n",
    "    df['csc_max']       = list(map(lambda x: x[3], token_features))\n",
    "    df['ctc_min']       = list(map(lambda x: x[4], token_features))\n",
    "    df['ctc_max']       = list(map(lambda x: x[5], token_features))\n",
    "    df['last_word_eq']  = list(map(lambda x: x[6], token_features))\n",
    "    df['first_word_eq'] = list(map(lambda x: x[7], token_features))\n",
    "    df['abs_len_diff']  = list(map(lambda x: x[8], token_features))\n",
    "    df['mean_len']      = list(map(lambda x: x[9], token_features))\n",
    "    \n",
    "    df['token_set_ratio']      = df.apply(lambda x: fuzz.token_set_ratio(x['question1'],x['question2']), axis=1)\n",
    "    df['token_sort_ratio']     = df.apply(lambda x: fuzz.token_sort_ratio(x['question1'],x['question2']), axis=1)\n",
    "    df['fuzz_ratio']           = df.apply(lambda x: fuzz.QRatio(x['question1'],x['question2']), axis=1)\n",
    "    df['fuzz_partial_ratio']   = df.apply(lambda x: fuzz.partial_ratio(x['question1'],x['question2']), axis=1)\n",
    "    df['longest_substr_ratio'] = df.apply(lambda x: get_longest_substr_ratio(x['question1'],x['question2']), axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('quora_fe_nlp.csv'):\n",
    "    df = pd.read_csv('quora_fe_nlp.csv', encoding='latin-1')\n",
    "    df.fillna('')\n",
    "else:\n",
    "    df = pd.read_csv('quora_train.csv', encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "    df = extract_features(df)\n",
    "    df.to_csv('quora_fe_nlp.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile('quora_train_p.txt'):\n",
    "    text_p = open('quora_train_p.txt').read()\n",
    "    text_n = open('quora_train_n.txt').read()\n",
    "else:\n",
    "    df_duplicate = df[df['is_duplicate']==1]\n",
    "    df_nonduplicate = df[df['is_duplicate']==0]\n",
    "\n",
    "    p = np.dstack([df_duplicate['question1'], df_duplicate['question2']]).flatten()\n",
    "    n = np.dstack([df_nonduplicate['question1'], df_nonduplicate['question2']]).flatten()\n",
    "\n",
    "    print('Number of data points in class 1: {}'.format(len(p)))\n",
    "    print('Number of data points in class 2: {}'.format(len(n)))\n",
    "\n",
    "    np.savetxt('quora_train_p.txt', p, delimiter=' ', fmt='%s')\n",
    "    np.savetxt('quora_train_n.txt', n, delimiter=' ', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_p = open('quora_train_p.txt').read()\n",
    "text_n = open('quora_train_n.txt').read()\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.add('said')\n",
    "stopwords.add('br')\n",
    "stopwords.add(' ')\n",
    "stopwords.remove('not')\n",
    "stopwords.remove('no')\n",
    "stopwords.remove('like')\n",
    "\n",
    "print('Total number of words in duplicate pair: {}'.format(len(text_p)))\n",
    "print('Total number of words in non duplicate pair: {}'.format(len(text_n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', max_words=len(text_p), stopwords=stopwords)\n",
    "wc.generate(text_p)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white', max_words=len(text_n), stopwords=stopwords)\n",
    "wc.generate(text_n)\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df[['ctc_min','cwc_min','csc_min','token_sort_ratio','is_duplicate']], hue='is_duplicate', vars=['ctc_min','cwc_min','csc_min','token_sort_ratio'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x='is_duplicate', y='token_sort_ratio', data=df)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate']==1]['token_sort_ratio'], color='red')\n",
    "sns.distplot(df[df['is_duplicate']==0]['token_sort_ratio'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.subplot(1,2,1)\n",
    "sns.violinplot(x='is_duplicate', y='fuzz_ratio', data=df)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "sns.distplot(df[df['is_duplicate']==1]['fuzz_ratio'], color='red')\n",
    "sns.distplot(df[df['is_duplicate']==0]['fuzz_ratio'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subsampled = df.head(10000)\n",
    "X = MinMaxScaler().fit_transform(df_subsampled[['cwc_min','cwc_max','csc_min','csc_max','ctc_min','ctc_max','last_word_eq','first_word_eq','abs_len_diff','mean_len','token_set_ratio','token_sort_ratio','fuzz_ratio','fuzz_partial_ratio','longest_substr_ratio']])\n",
    "y = df_subsampled['is_duplicate'].values\n",
    "\n",
    "tsne = TSNE(random_state=7).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'x':tsne[:,0], 'y':tsne[:,1], 'label':y})\n",
    "\n",
    "sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, size=8, palette='Set1', markers=['s', 'o'])\n",
    "plt.title('Perplexity: {} and Max-iter {}'.format(30,1000))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('quora_tfidf_q1.csv'):\n",
    "    df = pd.read_csv('quora_train.csv', encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "    questions = list(df['question1']) + list(df['question2'])\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit_transform(questions)\n",
    "    word_idf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    sample = nlp('Hi')\n",
    "    \n",
    "    vect_form_1 = []\n",
    "    for que_1 in tqdm_notebook(list(df['question1'])):\n",
    "        tokens_1 = nlp(que_1)\n",
    "        vec_1 = np.zeros([len(sample[0].vector)])\n",
    "        for token_1 in tokens_1:\n",
    "            wv_1 = token_1.vector\n",
    "            try:\n",
    "                idf=word_idf[str(token_1)]\n",
    "            except:\n",
    "                idf=0\n",
    "            vec_1 += wv_1*idf\n",
    "        vect_form_1.append(vec_1)\n",
    "    df['q1_vect_form'] = vect_form_1\n",
    "    df_q1 = pd.DataFrame(df['q1_vect_form'].values.tolist())\n",
    "    df_q1.to_csv('quora_tfidf_q1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('quora_tfidf_q2.csv'):\n",
    "    df = pd.read_csv('quora_train.csv', encoding='latin-1')\n",
    "    df = df.fillna('')\n",
    "    questions = list(df['question1']) + list(df['question2'])\n",
    "    tfidf = TfidfVectorizer()\n",
    "    tfidf.fit_transform(questions)\n",
    "    word_idf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    sample = nlp('Hi')\n",
    "    \n",
    "    vect_form_2 = []\n",
    "    for que_2 in tqdm_notebook(list(df['question2'])):\n",
    "        tokens_2 = nlp(que_2)\n",
    "        vec_2 = np.zeros([len(sample[0].vector)])\n",
    "        for token_2 in tokens_2:\n",
    "            wv_2 = token_2.vector\n",
    "            try:\n",
    "                idf=word_idf[str(token_2)]\n",
    "            except:\n",
    "                idf=0\n",
    "            vec_2 += wv_2*idf\n",
    "        vect_form_2.append(vec_2)\n",
    "    df['q2_vect_form'] = vect_form_2\n",
    "    df_q2 = pd.DataFrame(df['q2_vect_form'].values.tolist())\n",
    "    df_q2.to_csv('quora_tfidf_q2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('quora_final_features.csv'):\n",
    "    \n",
    "    df_fe_wo_pp = pd.read_csv('quora_fe_without_preprocessing.csv', encoding='latin-1')\n",
    "    \n",
    "    df_nlp = pd.read_csv('quora_fe_nlp.csv', encoding='latin-1')\n",
    "    df_nlp = df_nlp.drop(['qid1', 'qid2', 'question1', 'question2', 'is_duplicate'], axis=1)\n",
    "    \n",
    "    df = df_fe_wo_pp.merge(df_nlp, on='id', how='left')\n",
    "    \n",
    "    df_q1 = pd.read_csv('quora_tfidf_q1.csv', encoding='latin-1')\n",
    "    df_q1['id'] = df_fe_wo_pp['id']\n",
    "    \n",
    "    df = df.merge(df_q1, on='id', how='left')\n",
    "    \n",
    "    df_q2 = pd.read_csv('quora_tfidf_q2.csv', encoding='latin-1')\n",
    "    df_q2['id'] = df_fe_wo_pp['id']\n",
    "    \n",
    "    df = df.merge(df_q2, on='id', how='left')\n",
    "    \n",
    "    df.to_csv('quora_final_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
